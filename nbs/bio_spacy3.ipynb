{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElMegHe5SSkv"
      },
      "source": [
        "**Install Spacy 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1kyf5PKBSOc1"
      },
      "outputs": [],
      "source": [
        "# !pip install -U pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JLN1v5tKSXpv"
      },
      "outputs": [],
      "source": [
        "# !pip install -U spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-Y4DQhwySbjU"
      },
      "outputs": [],
      "source": [
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nl3HbZ7hScHx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.2.1                         \n",
            "Location         /home/diptesh/anaconda3/envs/acronymSpacy/lib/python3.8/site-packages/spacy\n",
            "Platform         Linux-5.4.0-91-generic-x86_64-with-glibc2.17\n",
            "Python version   3.8.12                        \n",
            "Pipelines        en_core_web_sm (3.2.0), en_core_web_trf (3.2.0)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# check you have the right version of Spacy. It should be spaCy version 3.2.1       \n",
        "\n",
        "!python -m spacy info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "weSPslYcShow"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap6bYlyvfgvE"
      },
      "source": [
        "**Convert train.txt to a spacy binary doc You should get a train.spacy file in your working dir**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "i5dnmYj6ffu5"
      },
      "outputs": [],
      "source": [
        "# !python -m spacy convert -c iob -s -b en_core_web_sm ./train.txt ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VpmXNPdxSnFg"
      },
      "outputs": [],
      "source": [
        "# Get spacy data for transformer\n",
        "\n",
        "# !python -m spacy download en_core_web_trf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mzXbrbbf-jZ"
      },
      "source": [
        "**Install CUDA 9.2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_3NSJc8pQlbr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-01-11 22:01:02--  https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 152.199.20.126\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|152.199.20.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://developer.nvidia.com/compute/cuda/9.2/prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 [following]\n",
            "--2022-01-11 22:01:11--  https://developer.nvidia.com/compute/cuda/9.2/prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64\n",
            "Reusing existing connection to developer.nvidia.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/9.2/secure/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb?xOEh7FsP2oD1WyXeKRjtb_o7pH8GmfyXGrGn8uLYGBwagvw_6uWmbJFrDnfB52TLvZTpISTzC0wo4VaJ-pEJcFh68sfByI-AH-y-Ye1FCEK0Enqkl9P2gyneE373QDhv-8ELlKpZM4fUN_pRM-FUFBgcQPfT0M9UCiWVyAJiniCsrmJ67jn0S0l-NLYzz9u15WV98C8oBCsWHUqbCJI [following]\n",
            "--2022-01-11 22:01:11--  https://developer.download.nvidia.com/compute/cuda/9.2/secure/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb?xOEh7FsP2oD1WyXeKRjtb_o7pH8GmfyXGrGn8uLYGBwagvw_6uWmbJFrDnfB52TLvZTpISTzC0wo4VaJ-pEJcFh68sfByI-AH-y-Ye1FCEK0Enqkl9P2gyneE373QDhv-8ELlKpZM4fUN_pRM-FUFBgcQPfT0M9UCiWVyAJiniCsrmJ67jn0S0l-NLYzz9u15WV98C8oBCsWHUqbCJI\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.20.126\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.20.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1267391958 (1.2G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604–9–2-local_9.2.88–1_amd64.deb’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   1.18G   112MB/s    in 11s     \n",
            "\n",
            "2022-01-11 22:01:30 (112 MB/s) - ‘cuda-repo-ubuntu1604–9–2-local_9.2.88–1_amd64.deb’ saved [1267391958/1267391958]\n",
            "\n",
            "[sudo] password for diptesh: \n",
            "[sudo] password for diptesh: "
          ]
        }
      ],
      "source": [
        "# !wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604–9–2-local_9.2.88–1_amd64.deb\n",
        "# !sudo dpkg -i cuda-repo-ubuntu1604–9–2-local_9.2.88–1_amd64.deb\n",
        "# !sudo apt-key add /var/cuda-repo-9–2-local/7fa2af80.pub\n",
        "# !sudo apt update\n",
        "# !sudo apt install cuda-9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "l3QLDB7zSqo1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu92\n",
            "  Downloading https://download.pytorch.org/whl/cu92/torch-1.7.1%2Bcu92-cp38-cp38-linux_x86_64.whl (577.3 MB)\n",
            "     |████████████████████████████████| 577.3 MB 15 kB/s               \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu92\n",
            "  Downloading https://download.pytorch.org/whl/cu92/torchvision-0.8.2%2Bcu92-cp38-cp38-linux_x86_64.whl (12.5 MB)\n",
            "     |████████████████████████████████| 12.5 MB 453 kB/s            \n",
            "\u001b[?25hCollecting torchaudio==0.7.2\n",
            "  Downloading torchaudio-0.7.2-cp38-cp38-manylinux1_x86_64.whl (7.6 MB)\n",
            "     |████████████████████████████████| 7.6 MB 7.3 kB/s            \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /home/diptesh/.local/lib/python3.8/site-packages (from torch==1.7.1+cu92) (3.10.0.0)\n",
            "Requirement already satisfied: numpy in /home/diptesh/anaconda3/envs/acronymSpacy/lib/python3.8/site-packages (from torch==1.7.1+cu92) (1.22.0)\n",
            "Collecting pillow>=4.1.1\n",
            "  Downloading Pillow-9.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "     |████████████████████████████████| 4.3 MB 33.2 MB/s            \n",
            "\u001b[?25hInstalling collected packages: torch, pillow, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.1\n",
            "    Uninstalling torch-1.10.1:\n",
            "      Successfully uninstalled torch-1.10.1\n",
            "Successfully installed pillow-9.0.0 torch-1.7.1+cu92 torchaudio-0.7.2 torchvision-0.8.2+cu92\n"
          ]
        }
      ],
      "source": [
        "# install Pytorch compatable with Spacy and Cuda version\n",
        "\n",
        "# !pip install torch==1.7.1+cu92 torchvision==0.8.2+cu92 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GTjOkBlwSxnR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: no matches found: spacy[cuda102,transformers]\n"
          ]
        }
      ],
      "source": [
        "# Change paths and install cupy\n",
        "# !pip install -U spacy[cuda102,transformers]\n",
        "# !pip install cupy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqIlKuiigbVr"
      },
      "source": [
        "**Write the path of train.spacy and valid.spacy in the base_config file**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W21EdpSgqar"
      },
      "source": [
        "**Fill in the Transformer parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BPZlPaStgkjh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYQgMI79g_Qp"
      },
      "source": [
        "# **TRAIN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "RgNzNNK_g4vK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: filt_deberta_base\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: filt_deberta_base\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-16 12:26:51,664] [INFO] Set up nlp object from config\n",
            "[2022-01-16 12:26:51,671] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2022-01-16 12:26:51,673] [INFO] Created vocabulary\n",
            "[2022-01-16 12:26:51,673] [INFO] Finished initializing nlp object\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2022-01-16 12:28:56,084] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0         781.61    476.55   59.46   58.63   60.32    0.59\n",
            "  0     200       50421.49  64852.90   93.04   93.43   92.66    0.93\n",
            "  0     400       12841.13  18336.84   94.41   95.10   93.73    0.94\n",
            "  0     600       10822.92  14849.30   94.73   95.01   94.45    0.95\n",
            "  1     800        9734.67  13393.18   94.94   95.12   94.76    0.95\n",
            "  1    1000        9446.74  12574.40   94.98   95.26   94.69    0.95\n",
            "  1    1200        9019.96  12228.04   95.00   95.72   94.29    0.95\n",
            "  2    1400        8890.85  11872.89   95.12   95.36   94.88    0.95\n",
            "  2    1600        8215.04  10817.59   95.18   95.53   94.83    0.95\n",
            "  2    1800        7852.96  10326.03   95.00   95.00   95.01    0.95\n",
            "  2    2000        7603.98  10207.28   95.14   95.29   95.00    0.95\n",
            "  3    2200        7214.85   9256.09   95.19   95.57   94.81    0.95\n",
            "  3    2400        7051.81   9433.61   95.26   95.91   94.61    0.95\n",
            "  3    2600        6944.85   9249.13   95.22   95.85   94.60    0.95\n",
            "  4    2800        6586.84   8620.98   95.16   95.21   95.11    0.95\n",
            "  4    3000        5885.60   8030.80   95.23   95.61   94.85    0.95\n",
            "  4    3200        6105.42   7959.22   95.22   95.58   94.87    0.95\n",
            "  4    3400        6088.90   8055.88   95.25   95.62   94.88    0.95\n",
            "  5    3600        5284.91   7252.05   95.30   95.76   94.84    0.95\n",
            "  5    3800        5214.51   7160.40   95.12   95.02   95.21    0.95\n",
            "  5    4000        5342.67   7193.28   95.11   95.05   95.17    0.95\n",
            "  6    4200        5297.99   7115.57   95.24   95.55   94.94    0.95\n",
            "  6    4400        4679.72   6567.71   95.29   95.56   95.03    0.95\n",
            "  6    4600        5059.60   6312.13   95.09   95.24   94.95    0.95\n",
            "  6    4800        5017.57   6536.58   94.95   94.84   95.06    0.95\n",
            "  7    5000        4480.79   6075.93   95.26   95.37   95.14    0.95\n",
            "  7    5200        3949.12   5524.48   95.19   95.21   95.17    0.95\n",
            "  7    5400        4018.23   5838.54   95.05   95.00   95.11    0.95\n",
            "  8    5600        4092.77   5699.61   95.21   95.62   94.80    0.95\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "filt_deberta_base/model-last\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy train -g 0 config.cfg --output ./filt_deberta_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: filt_distilbert_base\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: filt_distilbert_base\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-16 14:07:04,759] [INFO] Set up nlp object from config\n",
            "[2022-01-16 14:07:04,766] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2022-01-16 14:07:04,768] [INFO] Created vocabulary\n",
            "[2022-01-16 14:07:04,768] [INFO] Finished initializing nlp object\n",
            "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2022-01-16 14:09:06,728] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0         428.09    500.71   74.43   76.06   72.87    0.74\n",
            "  0     200       48728.25  79147.04   92.02   91.32   92.73    0.92\n",
            "  0     400       11991.81  23025.40   93.35   93.77   92.94    0.93\n",
            "  0     600        9537.50  18800.47   93.84   93.56   94.12    0.94\n",
            "  1     800        9071.99  17039.25   94.04   94.05   94.03    0.94\n",
            "  1    1000        8188.87  15554.20   94.18   94.25   94.11    0.94\n",
            "  1    1200        8003.59  15314.73   94.17   94.63   93.71    0.94\n",
            "  2    1400        7828.94  14662.17   94.31   94.45   94.16    0.94\n",
            "  2    1600        7003.91  13304.89   94.32   94.30   94.33    0.94\n",
            "  2    1800        7195.14  13203.60   94.07   93.99   94.15    0.94\n",
            "  2    2000        6751.15  12868.43   94.21   94.07   94.36    0.94\n",
            "  3    2200        6529.74  11786.22   94.39   94.76   94.02    0.94\n",
            "  3    2400        6238.00  11850.79   94.29   95.06   93.53    0.94\n",
            "  3    2600        6362.15  11650.06   94.45   94.64   94.26    0.94\n",
            "  4    2800        5870.17  10807.62   94.43   94.52   94.34    0.94\n",
            "  4    3000        5293.99  10132.07   94.49   94.63   94.34    0.94\n",
            "  4    3200        5305.17   9785.12   94.45   94.61   94.28    0.94\n",
            "  4    3400        5402.33  10184.47   94.45   94.48   94.42    0.94\n",
            "  5    3600        5093.09   9225.83   94.29   94.36   94.22    0.94\n",
            "  5    3800        4675.96   9022.02   94.42   94.56   94.28    0.94\n",
            "  5    4000        4837.70   9014.45   94.37   94.45   94.29    0.94\n",
            "  6    4200        4679.72   9115.87   94.48   94.76   94.20    0.94\n",
            "  6    4400        4132.45   8161.07   94.36   94.34   94.39    0.94\n",
            "  6    4600        4207.27   7921.80   94.41   94.38   94.44    0.94\n",
            "  6    4800        4456.11   8060.65   94.04   93.64   94.46    0.94\n",
            "  7    5000        3858.78   7423.63   94.44   94.70   94.18    0.94\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "filt_distilbert_base/model-last\n"
          ]
        }
      ],
      "source": [
        "import telegram\n",
        "bot = telegram.Bot(token=\"\")\n",
        "bot.sendMessage(chat_id=, text=\"Training is done\")\n",
        "\n",
        "!python -m spacy train -g 0 config2.cfg --output ./filt_distilbert_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<telegram.message.Message at 0x55a1b69a52c0>"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import telegram\n",
        "bot = telegram.Bot(token=\"5017487958:AAEtlP_pgluZe5wRNL_NASP1owuZZOVLzVo\")\n",
        "bot.sendMessage(chat_id=1600389501, text=\"Training is done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: filt_roberta_base\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: filt_roberta_base\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-16 15:03:47,084] [INFO] Set up nlp object from config\n",
            "[2022-01-16 15:03:47,090] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2022-01-16 15:03:47,092] [INFO] Created vocabulary\n",
            "[2022-01-16 15:03:47,093] [INFO] Finished initializing nlp object\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2022-01-16 15:05:58,356] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0         138.54    280.55   86.60   83.49   89.96    0.87\n",
            "  0     200       26372.77  52792.23   93.51   94.31   92.73    0.94\n",
            "  0     400       10581.66  18876.61   94.36   94.82   93.91    0.94\n",
            "  0     600        8768.19  15927.70   94.65   94.86   94.45    0.95\n",
            "  1     800        7907.37  14481.22   94.82   95.22   94.42    0.95\n",
            "  1    1000        7694.01  13747.33   94.86   95.09   94.63    0.95\n",
            "  1    1200        7326.81  13388.54   94.76   95.50   94.03    0.95\n",
            "  2    1400        7325.46  12906.59   95.08   95.62   94.55    0.95\n",
            "  2    1600        6618.17  11851.26   95.06   95.52   94.61    0.95\n",
            "  2    1800        6619.66  11676.55   94.76   95.03   94.50    0.95\n",
            "  2    2000        6336.91  11439.11   95.03   95.35   94.70    0.95\n",
            "  3    2200        5882.26  10365.53   95.08   95.43   94.74    0.95\n",
            "  3    2400        5688.29  10576.23   95.14   95.50   94.78    0.95\n",
            "  3    2600        5627.77  10435.67   95.06   95.27   94.85    0.95\n",
            "  4    2800        5387.50   9665.82   95.15   95.57   94.73    0.95\n",
            "  4    3000        4746.75   8984.66   95.14   95.58   94.70    0.95\n",
            "  4    3200        5148.21   9074.47   95.10   95.35   94.85    0.95\n",
            "  4    3400        4941.31   9287.69   95.15   95.20   95.10    0.95\n",
            "  5    3600        4666.40   8411.95   94.99   95.21   94.77    0.95\n",
            "  5    3800        4466.58   8274.57   95.00   94.88   95.12    0.95\n",
            "  5    4000        4543.53   8377.71   95.04   95.01   95.07    0.95\n",
            "  6    4200        4275.26   8125.46   95.15   95.35   94.95    0.95\n",
            "  6    4400        3692.13   7488.47   95.16   95.47   94.85    0.95\n",
            "  6    4600        3735.38   7236.48   94.85   94.72   94.98    0.95\n",
            "  6    4800        4033.60   7554.53   94.64   94.32   94.97    0.95\n",
            "  7    5000        3880.18   7148.51   95.15   95.39   94.90    0.95\n",
            "  7    5200        3184.40   6480.19   95.17   95.27   95.08    0.95\n",
            "  7    5400        3595.99   6924.20   95.00   94.98   95.03    0.95\n",
            "  8    5600        3287.76   6592.48   95.07   95.46   94.69    0.95\n",
            "  8    5800        3163.40   6305.19   94.75   94.50   95.00    0.95\n",
            "  8    6000        3256.63   6275.93   95.10   95.22   94.97    0.95\n",
            "  8    6200        2928.81   6145.65   95.16   95.07   95.26    0.95\n",
            "  9    6400        2663.99   5731.67   95.06   95.05   95.06    0.95\n",
            "  9    6600        2921.39   5980.00   95.00   94.95   95.06    0.95\n",
            "  9    6800        2656.65   5574.45   95.04   94.88   95.20    0.95\n",
            " 10    7000        2927.16   5673.95   95.11   95.14   95.08    0.95\n",
            " 10    7200        2512.35   5130.68   94.94   94.74   95.15    0.95\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "filt_roberta_base/model-last\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<telegram.message.Message at 0x55a1b6a97340>"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!python -m spacy train -g 0 config3.cfg --output ./filt_roberta_base\n",
        "import telegram\n",
        "bot = telegram.Bot(token=\"5017487958:AAEtlP_pgluZe5wRNL_NASP1owuZZOVLzVo\")\n",
        "bot.sendMessage(chat_id=1600389501, text=\"Final model training is done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   96.20\n",
            "NER R   94.91\n",
            "NER F   95.55\n",
            "SPEED   3240 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    97.91   95.90   96.90\n",
            "AC   83.82   90.09   86.84\n",
            "LF   76.36   77.84   77.09\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to output_rob_lar_final.json\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   95.85\n",
            "NER R   94.82\n",
            "NER F   95.33\n",
            "SPEED   14548\n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    97.52   95.99   96.75\n",
            "AC   83.99   88.28   86.08\n",
            "LF   75.07   76.17   75.62\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to output_rob_base_final.json\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   95.10\n",
            "NER R   94.42\n",
            "NER F   94.76\n",
            "SPEED   22677\n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    97.02   96.00   96.51\n",
            "AC   82.74   86.62   84.64\n",
            "LF   68.16   67.37   67.77\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to output_distilbert_base_final.json\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   96.04\n",
            "NER R   95.24\n",
            "NER F   95.64\n",
            "SPEED   12348\n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    97.62   96.44   97.03\n",
            "AC   85.38   87.88   86.61\n",
            "LF   75.21   77.03   76.11\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to output_deberta_base_final.json\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   95.46\n",
            "NER R   95.08\n",
            "NER F   95.27\n",
            "SPEED   2414 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    97.07   96.55   96.81\n",
            "AC   84.00   87.03   85.49\n",
            "LF   73.86   71.49   72.66\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to output_bert_large_final.json\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   95.41\n",
            "NER R   94.55\n",
            "NER F   94.98\n",
            "SPEED   13343\n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    97.10   96.13   96.61\n",
            "AC   84.07   85.00   84.53\n",
            "LF   71.85   70.53   71.18\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to output_bert_base_final.json\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   94.95\n",
            "NER R   94.32\n",
            "NER F   94.63\n",
            "SPEED   3127 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    96.51   96.12   96.31\n",
            "AC   84.16   83.51   83.83\n",
            "LF   71.84   66.72   69.19\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to output_albert_large_final.json\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   94.40\n",
            "NER R   94.31\n",
            "NER F   94.36\n",
            "SPEED   10803\n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    95.88   96.31   96.09\n",
            "AC   83.43   82.57   83.00\n",
            "LF   72.21   63.34   67.48\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to output_albert_base_final.json\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   95.61\n",
            "NER R   95.24\n",
            "NER F   95.43\n",
            "SPEED   14529\n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    97.33   96.49   96.91\n",
            "AC   82.78   89.20   85.87\n",
            "LF   75.15   73.78   74.46\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to filt_roberta_base.json\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   96.32\n",
            "NER R   94.57\n",
            "NER F   95.44\n",
            "SPEED   3225 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    98.28   95.37   96.80\n",
            "AC   82.62   89.91   86.11\n",
            "LF   74.97   82.10   78.37\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to filt_rob_large.json\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   94.94\n",
            "NER R   94.52\n",
            "NER F   94.73\n",
            "SPEED   22474\n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    97.04   95.95   96.49\n",
            "AC   80.33   89.36   84.60\n",
            "LF   68.71   66.82   67.75\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to filt_distilbert_base.json\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   96.03\n",
            "NER R   95.07\n",
            "NER F   95.55\n",
            "SPEED   12302\n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    97.95   96.07   97.00\n",
            "AC   82.99   90.14   86.42\n",
            "LF   73.40   78.18   75.71\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to filt_deberta_base.json\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   95.86\n",
            "NER R   94.54\n",
            "NER F   95.20\n",
            "SPEED   3212 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    97.98   95.58   96.77\n",
            "AC   82.19   88.88   85.41\n",
            "LF   70.56   77.53   73.88\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to filt_bert_large.json\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   95.37\n",
            "NER R   94.52\n",
            "NER F   94.94\n",
            "SPEED   13297\n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    97.52   95.77   96.64\n",
            "AC   80.99   89.39   84.98\n",
            "LF   69.11   71.33   70.20\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to filt_bert_base.json\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   94.83\n",
            "NER R   94.11\n",
            "NER F   94.47\n",
            "SPEED   3136 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    96.38   95.98   96.18\n",
            "AC   83.65   81.76   82.69\n",
            "LF   72.35   67.43   69.81\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to filt_albert_large.json\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   94.93\n",
            "NER R   94.57\n",
            "NER F   94.75\n",
            "SPEED   10784\n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R       F\n",
            "O    96.76   96.24   96.50\n",
            "AC   81.96   85.97   83.91\n",
            "LF   70.24   66.66   68.40\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to filt_albert_base.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy evaluate -g 0 ../nbs/output_rob_lar_final/model-best ../data/test_task.spacy --output ./output_rob_lar_final_ST.json\n",
        "!python -m spacy evaluate -g 0 ../nbs/output_rob_base_final/model-best ../data/test_task.spacy --output ./output_rob_base_final_ST.json\n",
        "!python -m spacy evaluate -g 0 ../nbs/output_distilbert_base_final/model-best ../data/test_task.spacy --output ./output_distilbert_base_final_ST.json\n",
        "!python -m spacy evaluate -g 0 ../nbs/output_deberta_base_final/model-best ../data/test_task.spacy --output ./output_deberta_base_final_ST.json\n",
        "!python -m spacy evaluate -g 0 ../nbs/output_bert_large_final/model-best ../data/test_task.spacy --output ./output_bert_large_final_ST.json\n",
        "!python -m spacy evaluate -g 0 ../nbs/output_bert_base_final/model-best ../data/test_task.spacy --output ./output_bert_base_final_ST.json\n",
        "!python -m spacy evaluate -g 0 ../nbs/output_albert_large_final/model-best ../data/test_task.spacy --output ./output_albert_large_final_ST.json\n",
        "!python -m spacy evaluate -g 0 ../nbs/output_albert_base_final/model-best ../data/test_task.spacy --output ./output_albert_base_final_ST.json\n",
        "!python -m spacy evaluate -g 0 ../nbs/filt_roberta_base/model-best ../data/test_task.spacy --output ./filt_roberta_base_ST.json\n",
        "!python -m spacy evaluate -g 0 ../nbs/filt_rob_large/model-best ../data/test_task.spacy --output ./filt_rob_large_ST.json\n",
        "!python -m spacy evaluate -g 0 ../nbs/filt_distilbert_base/model-best ../data/test_task.spacy --output ./filt_distilbert_base_ST.json\n",
        "!python -m spacy evaluate -g 0 ../nbs/filt_deberta_base/model-best ../data/test_task.spacy --output ./filt_deberta_base_ST.json\n",
        "!python -m spacy evaluate -g 0 ../nbs/filt_bert_large/model-best ../data/test_task.spacy --output ./filt_bert_large_ST.json\n",
        "!python -m spacy evaluate -g 0 ../nbs/filt_bert_base/model-best ../data/test_task.spacy --output ./filt_bert_base_ST.json\n",
        "!python -m spacy evaluate -g 0 ../nbs/filt_albert_large/model-best ../data/test_task.spacy --output ./filt_albert_large_ST.json\n",
        "!python -m spacy evaluate -g 0 ../nbs/filt_albert_base/model-best ../data/test_task.spacy --output ./filt_albert_base_ST.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "bio_spacy3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
